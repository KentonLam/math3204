\documentclass[12pt,a4paper]{article} % Default style and margins

% \usepackage[a4,center,noinfo,cross, width=16.5cm,height=24.5cm]{crop}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{listings}

\newtheorem*{lemma}{Lemma}
\newtheorem*{theorem}{Theorem}
\newtheorem*{definition}{Definition}

\DeclareMathOperator{\Rank}{Rank}
\DeclareMathOperator{\Colsp}{colsp}
\DeclareMathOperator{\Rowsp}{rowsp}
\DeclareMathOperator{\Range}{Range}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\Null}{Null}

\title{Krylov Subspace Methods}
\author{Kenton Lam}

\begin{document}
\maketitle

This document aims to describe Krylov subspace methods, 
one of the best options for solving linear systems of equations.
Briefly, they have the following desirable properties:
\begin{itemize}
    \item No explicit form of $\mathbf A$ is needed; only a matrix-vector product is required.
    \item Well-suited for large and sparse systems.
    \item Optimised variations of Krylov methods are available for specific matrix types.
    \item For approximate solutions, Krylov methods have good convergence/approximation properties.
\end{itemize}
This is based on MATH3204 lectures and notes, lectured by Fred Roosta. 
The lecure slides contain proofs of some theorems not proved here.

\section{Motivation}
The general form of a linear system is 
\begin{align*}
    \mathbf A \bm v^\star &= \bm b
\end{align*}
where $\mathbf A \in \mathbb C^{ \times n}$ and $\mathbf A$ is invertible. 
Assume $\rho(\mathbf I - \mathbf A) < 1$. Then, we can write
$\mathbf A^{-1}$ as a geometric series,
\begin{align*}
    \mathbf A^{-1} = (\mathbf I - (\mathbf I - \mathbf A))^{-1} = \sum_{k=0}^\infty (\mathbf I-\mathbf A)^k.
\end{align*}
Suppose we have an initial guess $\bm x_0 \in \mathbb C^n$.
Define the residual of this guess as $\bm r_0 = \mathbf A \bm x^\star - \mathbf A \bm x_0$.
Then,
\begin{align*}
    \bm x^\star &= \mathbf A^{-1}\bm b = \mathbf A^{-1} 
    (\mathbf A \bm x^\star - \mathbf A \bm x_0 + \mathbf A \bm x_0) \\ 
    &= \bm x_0 + \mathbf A^{-1} \bm r_0 = \bm x_0 + \sum_{k=0}^\infty (\mathbf I-\mathbf A)^k \bm r_0
\end{align*}
This is great, but largely useless if we need to compute infinitely many vectors 
to find $\bm x^\star$. However it turns out that we actually don't need to.
\begin{theorem}[Cayley--Hamilton]
    Let $p_n(\lambda) = \sum_{i=0}^n c_i \lambda^i$ be the characteristic 
    polynomial of the matrix $\mathbf A$. Then, $p_n(\mathbf A) = 0$.
\end{theorem}
This implies that $\mathbf A^{-1}$ can be written as a finite sum of linear 
combinations of powers of $\mathbf A$. Specifically, it is a matrix polynomial 
of degree at most $n-1$. As a result, 
\begin{align*}
    \bm x^\star \in \bm x_0 + \Span \left\{ \bm r_0, \mathbf A \bm r_0, \ldots, \mathbf A^{n-1}\rm r_0\right\}.
\end{align*}
Suppose we only consider a subspace of this, so choose $k < n$
\begin{align*}
    \bm x_k \in \bm x_0 + \Span \left\{ \bm r_0, \mathbf A \bm r_0, \ldots, \mathbf A^{k-1}\rm r_0\right\}.
\end{align*}
This is the central question of Kylov methods. How good is the approximation $\bm x_k$ to 
$\bm x^\star$? What does ``good'' even mean?

In fact, Richardson iterations is a case of these subspace approximations. In Richardson, 
\begin{align*}
    \bm x_k = \bm x_0 + \sum_{i=0}^{k-1} \alpha_i \prod_{j=0}^{i-1} (\mathbf I - \alpha_j \mathbf A) \bm r_0
\end{align*}
However, depending on our choice of $\alpha_k$, we saw dramatically different convergence.

The great quest of Krylov subspace methods is to find the the ``best'' 
(in some sense)
$\bm x_k \approx \bm x^\star$ for some $k \ll n$.

\begin{definition}[Krylov Subspace]
    The Krylov subspace of order $k$, generated by the matrix $\mathbf A$ and 
    vector $\bm v$ is defined as 
    \begin{align*}
        \mathcal K_k(\mathbf A, \bm v) = \Span \left\{ \bm v, \mathbf A \bm v, 
        \ldots, \mathbf A^{k-1}\bm v\right\}
    \end{align*}
    for $k \ge 1$ and $\mathcal K_0(\mathbf A, \bm v) = \left\{ \bm 0\right\}$.
\end{definition}
Because these subspaces are nested, their dimensions cannot grow indefinitely.
At some point, the Krylov subspace will be large enough that it 
``contains'' all the information we can extract from $\mathbf A$ through its 
multiplication by $\bm v$. Consider the simplest case when $\bm v$ is an eigenvector, 
then the Kyrlov space just has dimension 1 for all $k$.
\begin{theorem}[Grade of $\bm v$ with respect to $\mathbf A$]
    There exists a positive integer $t = t(\bm v, \mathbf A)$, the grade of 
    $\bm v$ with respect to $\mathbf A$ such that 
    \begin{align*}
        \dim \mathcal K_k (\mathbf A, \bm v) = \min \left\{ k, t\right\}.
    \end{align*}
\end{theorem}
This means that for any $k \le t$, all the generated vectors are linearly independent. After 
$t$, the new vectors are linearly dependent on the previous ones. This means that 
for $k > t$, $\mathcal K_k (\mathbf A, \bm v) = \mathcal K_{k+1} (\mathbf A, \bm v)$.
As a direct corollary of this, 
\begin{align*}
    t = \min \left\{ k ~|~ \mathbf A^{-1}\bm v \in \mathcal K_k (\mathbf A, \bm v)\right\}.
\end{align*}
Recall that initially we had 
$\bm x^\star \in \bm x_0 + \mathcal K_n (\mathbf A, \bm r_0)$.
Now, we have a more specific result that 
\begin{align*}
    \bm x^\star \in \bm x_0 + \mathcal K_t (\mathbf A, \bm r_0)
\end{align*}
where $\bm r_0 = \bm b - \mathbf A - \bm x_0$ and $t$ is the grade of 
$\bm r_0$ with respect to $\mathbf A$.

To summarise, standard Krylov subspace solvers can be descibed as follows.
\begin{definition}[Standard Krylov Subspace Method]
    A standard Krylov subspace method is an {iterative method}, 
    which starting from some $\bm x_0$, 
    generates an appropriate sequence of iterates 
    $\bm x_k \in \bm x_0 + \mathcal K_k (\mathbf A, \bm r_0)$, 
    until it finds $\bm x^\star$ in exactly $t$ steps.

    The iterates are chosen appropriately such that if we terminate
    early, we have still $\bm x_k \approx \bm x^\star$ in some sense.
\end{definition}
Note that not all Krylov methods are of this form. Some are bulid upon 
different types of subspace or work with multiple subspaces (e.g.\ they 
also consider $\mathcal K_t (\mathbf A^*, \bm w)$).

Many terms are intentionally left vague in the above definition, because 
Krylov subspace solders differ among themselves in many aspects, such as 
\begin{itemize}
    \item the underlying Krylov subspace, 
    \item the method in which $\bm x_k$ is chosen, and
    \item the sense in which $\bm x_k \approx \bm x^\star$ is measured.
\end{itemize}
Additionally, in exact arithmetic, Krylov methods have finite termination property 
(i.e.\ they will always finish with an exact solution in finite iterations). 
Unfortunately, this does not hold in finite-precision
 arithmetic, such as on a computer.
\end{document}