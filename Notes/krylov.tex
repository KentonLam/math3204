\documentclass[12pt,a4paper]{article} % Default style and margins

% \usepackage[a4,center,noinfo,cross, width=16.5cm,height=24.5cm]{crop}


\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bm}
\usepackage{listings}

\newtheorem*{lemma}{Lemma}
\newtheorem*{theorem}{Theorem}
\newtheorem*{proposition}{Proposition}
\newtheorem*{definition}{Definition}

\DeclareMathOperator{\Rank}{Rank}
\DeclareMathOperator{\Colsp}{colsp}
\DeclareMathOperator{\Rowsp}{rowsp}
\DeclareMathOperator{\Range}{Range}
\DeclareMathOperator{\Span}{Span}
\DeclareMathOperator{\Null}{Null}

\title{Krylov Subspace Methods}
\author{Kenton Lam}

\begin{document}
\maketitle

These notes aim to describe Krylov subspace methods, 
one of the best options for solving linear systems of equations.
Briefly, they have the following desirable properties:
\begin{itemize}
    \item No explicit form of $\mathbf A$ is needed; only a matrix-vector product is required.
    \item Well-suited for large and sparse systems.
    \item Optimised variations of Krylov methods are available for specific matrix types.
    \item For approximate solutions, Krylov methods have good convergence/approximation properties.
\end{itemize}
This is based on MATH3204 lectures and notes, lectured by Fred Roosta. 
These notes are meant to give an intuitive understanding of the topic and 
omit many proofs which can be found in the lecture slides.

\section{Introduction}
The general form of a linear system is 
\begin{align*}
    \mathbf A \bm x^\star &= \bm b
\end{align*}
where $\mathbf A \in \mathbb C^{ \times n}$ and $\mathbf A$ is invertible. 
Assume $\rho(\mathbf I - \mathbf A) < 1$. Then, we can write
$\mathbf A^{-1}$ as a geometric series,
\begin{align*}
    \mathbf A^{-1} = (\mathbf I - (\mathbf I - \mathbf A))^{-1} = \sum_{k=0}^\infty (\mathbf I-\mathbf A)^k.
\end{align*}
Suppose we have an initial guess $\bm x_0 \in \mathbb C^n$.
Define the residual of this guess as $\bm r_0 = \mathbf A \bm x^\star - \mathbf A \bm x_0$.
Then,
\begin{align*}
    \bm x^\star &= \mathbf A^{-1}\bm b = \mathbf A^{-1} 
    (\mathbf A \bm x^\star - \mathbf A \bm x_0 + \mathbf A \bm x_0) \\ 
    &= \bm x_0 + \mathbf A^{-1} \bm r_0 = \bm x_0 + \sum_{k=0}^\infty (\mathbf I-\mathbf A)^k \bm r_0
\end{align*}
This is great, but largely useless if we need to compute infinitely many vectors 
to find $\bm x^\star$. However it turns out that we actually don't need to.
\begin{theorem}[Cayley--Hamilton]
    Let $p_n(\lambda) = \sum_{i=0}^n c_i \lambda^i$ be the characteristic 
    polynomial of the matrix $\mathbf A$. Then, $p_n(\mathbf A) = 0$.
\end{theorem}
This implies that $\mathbf A^{-1}$ can be written as a finite sum of linear 
combinations of powers of $\mathbf A$. Specifically, it is a matrix polynomial 
of degree at most $n-1$. As a result, 
\begin{align*}
    \bm x^\star \in \bm x_0 + \Span \left\{ \bm r_0, \mathbf A \bm r_0, \ldots, \mathbf A^{n-1}\rm r_0\right\}.
\end{align*}
Suppose we only consider a subspace of this, so choose $k < n$
\begin{align*}
    \bm x_k \in \bm x_0 + \Span \left\{ \bm r_0, \mathbf A \bm r_0, \ldots, \mathbf A^{k-1}\rm r_0\right\}.
\end{align*}
This is the central question of Kylov methods. How good is the approximation $\bm x_k$ to 
$\bm x^\star$? What does ``good'' even mean?

In fact, Richardson iterations is a case of these subspace approximations. In Richardson, 
\begin{align*}
    \bm x_k = \bm x_0 + \sum_{i=0}^{k-1} \alpha_i \prod_{j=0}^{i-1} (\mathbf I - \alpha_j \mathbf A) \bm r_0
\end{align*}
However, depending on our choice of $\alpha_k$, we saw dramatically different convergence.

The great quest of Krylov subspace methods is to find the the ``best'' 
(in some sense)
$\bm x_k \approx \bm x^\star$ for some $k \ll n$.

\begin{definition}[Krylov Subspace]
    The Krylov subspace of order $k$, generated by the matrix $\mathbf A$ and 
    vector $\bm v$ is defined as 
    \begin{align*}
        \mathcal K_k(\mathbf A, \bm v) = \Span \left\{ \bm v, \mathbf A \bm v, 
        \ldots, \mathbf A^{k-1}\bm v\right\}
    \end{align*}
    for $k \ge 1$ and $\mathcal K_0(\mathbf A, \bm v) = \left\{ \bm 0\right\}$.
\end{definition}
Because these subspaces are nested, their dimensions cannot grow indefinitely.
At some point, the Krylov subspace will be large enough that it 
``contains'' all the information we can extract from $\mathbf A$ through its 
multiplication by $\bm v$. Consider the simplest case when $\bm v$ is an eigenvector, 
then the Kyrlov space just has dimension 1 for all $k$.
\begin{theorem}[Grade of $\bm v$ with respect to $\mathbf A$]
    There exists a positive integer $t = t(\bm v, \mathbf A)$, the grade of 
    $\bm v$ with respect to $\mathbf A$ such that 
    \begin{align*}
        \dim \mathcal K_k (\mathbf A, \bm v) = \min \left\{ k, t\right\}.
    \end{align*}
\end{theorem}
This means that for any $k \le t$, all the generated vectors are linearly independent. After 
$t$, the new vectors are linearly dependent on the previous ones. This means that 
for $k > t$, $\mathcal K_k (\mathbf A, \bm v) = \mathcal K_{k+1} (\mathbf A, \bm v)$.
As a direct corollary of this, 
\begin{align*}
    t = \min \left\{ k ~|~ \mathbf A^{-1}\bm v \in \mathcal K_k (\mathbf A, \bm v)\right\}.
\end{align*}
Recall that initially we had 
$\bm x^\star \in \bm x_0 + \mathcal K_n (\mathbf A, \bm r_0)$.
Now, we have a more specific result that 
\begin{align*}
    \bm x^\star \in \bm x_0 + \mathcal K_t (\mathbf A, \bm r_0)
\end{align*}
where $\bm r_0 = \bm b - \mathbf A - \bm x_0$ and $t$ is the grade of 
$\bm r_0$ with respect to $\mathbf A$.

To summarise, standard Krylov subspace solvers can be descibed as follows.
\begin{definition}[Standard Krylov Subspace Method]
    A standard Krylov subspace method is an {iterative method}, 
    which starting from some $\bm x_0$, 
    generates an appropriate sequence of iterates 
    $\bm x_k \in \bm x_0 + \mathcal K_k (\mathbf A, \bm r_0)$
    until it finds $\bm x^\star$ in exactly $t$ steps.

    The iterates are chosen appropriately such that if we terminate
    early, we have still $\bm x_k \approx \bm x^\star$ in some sense.
\end{definition}
Note that not all Krylov methods are of this form. Some are bulid upon 
different types of subspace or work with multiple subspaces (e.g.\ they 
also consider $\mathcal K_k (\mathbf A^*, \bm w)$).

Many terms are intentionally left vague in the above definition, because 
Krylov subspace solders differ among themselves in many aspects, such as 
\begin{itemize}
    \item the underlying Krylov subspace, 
    \item the method in which $\bm x_k$ is chosen, and
    \item the sense in which $\bm x_k \approx \bm x^\star$ is measured.
\end{itemize}
Additionally, in exact arithmetic, Krylov methods have finite termination property 
(i.e.\ they will always finish with an exact solution in finite iterations). 
Unfortunately, this does not hold in finite-precision
 arithmetic, such as on a computer.

\section{Computing a Basis}
\subsection{Motivation}
How do we construct vectors from some vector space? With a basis for that 
space, of course!

Suppose the grade of $\bm r_0$ w.r.t.\ $\mathbf A$ is $n$, so the basis matrix 
\begin{align*}
    \mathbf K = \begin{bmatrix}
        \bm r_0 & \mathbf A \bm r_0 & \cdots & \mathbf A^{n-1}\bm r_0
    \end{bmatrix} \in \mathbb R^{n \times n}
\end{align*}
is invertible. Then, 
\begin{align*}
    \mathbf A \mathbf K &= \begin{bmatrix}
        \mathbf A\bm r_0 & \mathbf A^2 \bm r_0 & \cdots & \mathbf A^{n}\bm r_0
    \end{bmatrix} \\ 
    &= \mathbf K \underbrace{\begin{bmatrix}
        \bm e_2 & \bm e_3 & \cdots & \bm e_n & \mathbf K^{-1} \mathbf A^n \bm r_0
    \end{bmatrix}}_{\mathbf C \in \mathbb R^{n \times n}}
\end{align*}
By construction, $\mathbf K^{-1} \mathbf A \mathbf K = \mathbf C$. It can be seen that 
$\mathbf C$ is an $n \times n$ matrix and upper Hessenberg.
Although $\mathbf C$ is sparse and easy to work with, such a basis is practically
useless for our purposes. 
\begin{itemize}
    \item Because $\mathbf C$ is $n \times n$, we need $n$ matrix-vector products.
    \item $\mathbf K$ could be very dense even if $\mathbf A$ is sparse. 
    \item $\mathbf K$ is ill-conditioned.
\end{itemize}

Suppose we take the $\mathbf Q \mathbf R$ decomposition of $\mathbf K$, so 
$\mathbf {K} = \mathbf {QR}$ where $\mathbf Q$ is orthogonal and $\mathbf R$ is 
upper triangular. Then, 
\begin{align*}
    \mathbf{Q}^{\top} \mathbf{A} \mathbf{Q}=\mathbf{R} \mathbf{K}^{-1} \mathbf{A} \mathbf{K} \mathbf{R}^{-1}=\mathbf{R} \mathbf{C} \mathbf{R}^{-1} = \mathbf{H}
\end{align*}
where $\mathbf H$ is an upper Hessenberg matrix. 
It can be seen that $\Range \mathbf K$ is the same as $\Range \mathbf Q$,
 so $\mathbf Q$ also 
spans our Krylov subspace.

For the subspace $\mathcal K_k (\mathbf A, \bm r_0)$, $k \ll n$, 
we search for $\mathbf Q_k \in \mathbb R^{n \times k}$ such that 
\begin{align*}
    \mathbf Q_k^\top \mathbf A \mathbf Q_k = \mathbf H_k \in \mathbb R^{k \times k}
\end{align*}
is upper Hessenberg. Note that this $\mathbf H_k$ is only $k \times k$ so 
all our computations can be done with this smaller matrix. To summarise, 
we aim to find $\mathbf Q_k$ with the following properties:
\begin{itemize}
    \item The columns  of $\mathbf Q_k$ form an orthonormal basis of $\mathcal K_k (\mathbf A, \bm r_0)$.
    \item $\mathbf Q_k^\top \mathbf A \mathbf Q_k = \mathbf H_k$ is upper Hessenberg.
\end{itemize}
In general, $\mathbf A \mathbf Q_k \ne \mathbf Q_k \mathbf H_k$ for any $k < n$.
Why is this so? Suppose we left-multiply $\mathbf Q_k^\top \mathbf A \mathbf Q_k = \mathbf H_k$
 by $\mathbf Q_k$. 
If this was orthgonal, we'd have $\mathbf Q_k \mathbf Q_k^\top = \mathbf I_n$.
However, the matrix product  $\mathbf Q_k \mathbf Q_k^\top$ is essentially 
a map $\mathbb R^n \to \mathbb R^k \to \mathbb R^n$. If this is identity, 
it implies a $k$-dimensional set can span an $n$-dimensional space, 
which is absurd since $k < n$.

To get equality, we adjust with an error term $\mathbf E_k \in \mathbb R^{n \times k}$,
\begin{align*}
    \mathbf A \mathbf Q_k = \mathbf Q_k \mathbf H_k + \mathbf E_k.
\end{align*}
In order to have $\mathbf Q_k^\top \mathbf A \mathbf Q_k = \mathbf H_k$ 
hold, we need $\mathbf Q_k^\top \mathbf E_k = \mathbf 0$.

Suppose we have a vector $\bm q_{k+1}$, orthogonal to all $q_i \le k$.
Then, if $\mathbf E_k = \bm q_{k+1} \bm h^\top_{k}$ for any $\bm h_k \in \mathbb R^n$. 
Because $\bm q_{k+1}$ is orthogonal to every column of $\mathbf Q_k^\top$, we 
see that $\mathbf Q_k^\top \mathbf E_k = \mathbf Q_k^\top \bm q_{k+1} \bm h^\top_{k} = \mathbf 0$. 
Because this holds for any $\bm h_k$, we choose $\bm h_k$ with zeros in all positions 
except the $k$-th, where it is $h_{k+1,k}$.

So $\mathbf{A} \mathbf{Q}_{k}=\mathbf{Q}_{k} \mathbf{H}_{k}+\bm{q}_{k+1} \bm{h}_{k}^{\top},$ and we can write
\begin{align*}
    \mathbf{A} \mathbf{Q}_{k}=
    \underbrace{\begin{bmatrix}
        \mathbf Q_k & \bm q_{k+1}
    \end{bmatrix}}_{\mathbf Q_{k+1}}
    \underbrace{\begin{bmatrix}\mathbf{H}_{k} \\[0.8em] \bm{h}_{k}^{\top}\end{bmatrix}}_{\mathbf H_{k+1,k}}, 
        \quad \text { where }\bm{h}_{k}^\top= \begin{bmatrix}
            0 & \cdots & 0 & h_{k+1,k}
        \end{bmatrix}.
\end{align*}
This gives us an expression for $\bm q_{k+1}$ given all the previous $\bm q_k$'s.

\subsection{Arnoldi Process}
The Arnoldi algorithm is a modified version of Gram-Schmidt which finds 
the desired $\mathbf Q_k$.

In the base case of $k=1$, 
we just have $\bm q_1 = \bm r_0 / \| \bm r_0 \|$.

For $k=2$, we have 
\begin{align*}
    \mathbf{A} \bm q_1=
    \begin{bmatrix}
        \bm q_1 & \bm q_2
    \end{bmatrix}
    \begin{bmatrix}
        h_{11} \\ h_{21}
    \end{bmatrix} \implies 
    \mathbf A \bm q_1 = h_{11} \bm q_1 + h_{21} \bm q_2.
\end{align*}
We apply the fact that $\mathbf Q_k$ must have orthonormal columns. 
\begin{align*}
    \bm q_1^\top\mathbf A \bm q_1 = h_{11} \bm q_1^\top\bm q_1 + h_{21} \bm q_1^\top\bm q_2&& \implies h_{11} &= \langle \bm q_1, \mathbf A \bm q_1\rangle \\ 
    \mathbf A \bm q_1 - h_{11} \bm q_1 = h_{21} \bm q_2&& \implies h_{21} &= \| \mathbf A \bm q_1 - h_{11} \bm q_1\| \\ 
    &&\implies \,\,\bm q_2 &= \frac{\mathbf A \bm q_1 - h_{11} \bm q_1}{h_{21}}
\end{align*}

Consider the general case of $k=j$. Observe that the $j$-th step adds 
one row and one column to $\mathbf H_{j+1,j}$, namely the $(j+1)$-th row and $j$-th 
column. As block matrices, this can be visualised as 
\begin{align*}
    \mathbf A \mathbf Q_j &= \begin{bmatrix}
        \mathbf Q_j & \bm q_{j+1}
    \end{bmatrix}
    \begin{bmatrix}
        \mathbf H_{j,j-1} & \begin{matrix}
            h_{1j} \\ \vdots \\ h_{jj}
        \end{matrix} \\ 
        \begin{matrix}
            0 & \cdots & 0
        \end{matrix} & h_{j+1,j}
    \end{bmatrix}.
\end{align*}
Because we only change the last column and row in this iteration, this can be reduced to 
\begin{align*}
    \mathbf A \bm q_j = h_{1j}\bm q_1 + h_{2j} \bm q_2 + \cdots + h_{j+1,j}\bm q_{j+1}.
\end{align*}
Through a similar process to the $k=2$ case, we have 
\begin{align*}
    h_{ij} &= \langle \bm q_i, \mathbf A\bm q_j \rangle \quad \text{for } i=1, \ldots, j \\ 
    h_{j+1,j} &= \| \mathbf A \bm q_j - \textstyle\sum_{i=1}^j h_{ij} \bm q_i \| \\ 
    \bm q_{j+1} &= \frac{\mathbf A \bm q_j - \sum_{i=1}^j h_{ij} \bm q_i }{h_{j+1,j}}
\end{align*}
From this, we can easily derive an algorithm for the Arnoldi process.
If at any point, $h_{j+1,j} = 0$, then the algorithm terminates. 

\subsection{Results}
\begin{theorem}
    Assume the Arnoldi process does not terminate before $k$ steps. Then the
vectors $\left\{\bm{q}_{1}, \bm{q}_{2}, \ldots, \bm{q}_{k}\right\}$ 
form an orthonormal basis for $\mathcal{K}_{k}\left(\mathbf{A}, \bm{r}_{0}\right)$.
\end{theorem}
However, if Arnoldi breaks down at step $j$, then that means $q_{j+1}$ can be written 
as a linear combination of the previous vectors. This should sound familiar. In fact,
we have a theorem.
\begin{theorem}
    The Arnoldi process breaks down at step $j$, i.e.\ $h_{j+1, j}=0$, if and only if the grade of $\bm{r}_{0}$
     w.r.t. $\mathbf{A}$ is $j$. That is, $t\left(\bm{r}_{0}, \bm{A}\right)=j$.
\end{theorem}
For this reason, we call this termination a ``lucky breakdown''.

\section{Optimality Criterion}
Among infinitely many vectors, 
what is the ``best'' $\bm x_k \in \bm x_0 + \mathcal K_k(\mathbf A, \bm r_0)$?

\textbf{Note.} For the remainder, we assume $\mathbf A \in \mathbb R^{n \times n}$
and $\bm b \in \mathbb R^n$ for simplicity. That is, everything is real-valued.

Some natural optimality conditions are 
\iffalse
\bm x_k = \underset{\bm x \in \bm x_0 + \mathcal K_k(\mathbf A, \bm r_0)}{\arg \min }
\fi
\begin{itemize}    
    \item $\bm x_k = \underset{\bm x \in \bm x_0 + \mathcal K_k(\mathbf A, \bm r_0)}{\arg \min } \| \bm x - \bm x^\star \|_2$.
    \begin{itemize}
        \item However, this is not quite possible since we don't know $\bm x^\star$.
    \end{itemize}
    \item For $\mathbf A \succ \mathbf 0$, minimise the energy norm:
    \begin{align*}
        \bm x_k = \underset{\bm x \in \bm x_0 + \mathcal K_k(\mathbf A, \bm r_0)}{\arg \min } \|\bm x - \bm x^\star\|_\mathbf A = \bm x_k = \underset{\bm x \in \bm x_0 + \mathcal K_k(\mathbf A, \bm r_0)}{\arg \min } \frac{1}{2} \langle \bm x, \mathbf A \bm x \rangle - \langle \bm b, \bm x \rangle.
    \end{align*}
    \begin{itemize}
        \item This is conjugate gradient (CG).
    \end{itemize}
    \item Minimise the residual:
    \begin{align*}
        \bm x_k = \underset{\bm x \in \bm x_0 + \mathcal K_k(\mathbf A, \bm r_0)}{\arg \min } \| \mathbf A \bm x - \bm b \|_2
    \end{align*}
    \begin{itemize}
        \item For symmetric $\mathbf A$, this is MINRES.
        \item For nonsymmetric $\mathbf A$, this is GMRES.
    \end{itemize}
\end{itemize}

\subsection{Projection Methods}
However, there is one condition which can unify many aspects of the analysis in 
a more general way for all Krylov methods:
\begin{itemize}
    \item Find $\bm x_k \in  \bm x_0 + \mathcal K_k(\mathbf A, \bm r_0)$ such that 
    $\bm r_k \perp \mathcal L_k$ where $\mathcal L_k$ is some $k$-dimensional subspace.
\end{itemize}
These are called \textit{projection methods}. The intuition behind projection 
methods is this:
\begin{itemize}
    \item Once a basis has been constructed, there are $k$ degrees of freedom
    for picking a point in a $k$-dimensional affine subspace.
    \item So, to uniquely determine a point, we need in general $k$ constraints.
    \item A typical way is to impose that the residual is orthogonal to $k$
    linearly independent vectors, e.g.\ the basis of another $k$-dimensional
    subspace.
\end{itemize}
Projection methods have the following ingredients: a \textbf{search subspace} 
$\mathcal K_k$, a \textbf{constraint subspace} $\mathcal L_k$ and the 
\textbf{Petrov-Galerkin conditions} $\bm x_k \in  \bm x_0 + \mathcal K_k$
such that $\bm r_k \perp \mathcal L_k$.

If $\mathcal K_k = \mathcal L_k$, this is orthogonal projection. If $\mathcal L_k \ne \mathcal K_k$, 
this is oblique projection.

\subsection{Imposing Conditions}
Let $\bm x_k = \bm x_0 + \bm z_k$ where $\bm x_k \in \mathcal K_k$.
How can we enforce these constraints? Specifically, we need
\begin{align*}
    \bm{z}_{k} \in \mathcal{K}_{k}
    \quad\text{and}\quad
    \left\langle\bm{r}_{0}-\mathbf{A} \bm{z}_{k}, \bm{w}\right\rangle= 0, ~\forall ~\bm{w} \in \mathcal{L}_{k}.
\end{align*}
Suppose $\mathbf K_k$ and $\mathbf L_k$ are bases for $\mathcal K_k$
and $\mathcal L_k$ respectively.  Then, 
\begin{align*}
    \bm{z}_{k} \in \mathcal{K}_{k} &\iff  \bm z_k = \mathbf K_k \bm y_k, \text{ for some } \bm y_k \in \mathbb R^k \\ 
    \bm r_0 - \mathbf A \bm z_k \perp \mathcal L_k &\iff \mathbf L_k^\top (\bm r_0 - \mathbf A \mathbf K_k \bm y_k) = \mathbf 0
\end{align*}
If $\mathbf L_k^\top \mathbf A \mathbf K_k$ is non-singular, we can express $\bm x_k$ as 
\begin{align*}
    \bm x_k = \bm x_0 + \mathbf K_k (\mathbf L_k^\top \mathbf A \mathbf K_k)^{-1} \mathbf L_k^\top \bm r_0.
\end{align*}
Note that this requires $\mathbf L_k^\top \mathbf A \mathbf K_k$ be non-singular, which 
can be violated even if $\mathbf A$ itself is non-singular.

\begin{proposition}
    $\mathbf L_k^\top \mathbf A \mathbf K_k$ is non-singular if 
    \begin{itemize}
        \item $\mathbf A \succ \bm 0$ and $\mathcal L = \mathcal K$, or 
        \item $\det \mathbf A \ne 0$ and $\mathcal L = \mathbf A \mathcal K$.
    \end{itemize}
\end{proposition}

\subsection{Relation to CG and GMRES}
\begin{theorem}
    The case where $\mathbf A \succ \bm 0$ and $\mathcal L_k = \mathcal K_k$
    is equivalent to CG, 
    \begin{align*}
        \bm x_k = \underset{\bm x \in \bm x_0 + \mathcal K_k(\mathbf A, \bm r_0)}{\arg \min } \| \bm x - \bm x^\star \|_\mathbf A = \bm x_k = \underset{\bm x \in \bm x_0 + \mathcal K_k(\mathbf A, \bm r_0)}{\arg \min } \frac{1}{2} \langle \bm x, \mathbf A \bm x \rangle - \langle \bm b, \bm x \rangle.
    \end{align*}
    The case where $\det \mathbf A \ne 0$ and $\mathcal L_k = \mathbf A \mathcal K_k$ 
    is equivalent to MINRES/GMRES,
    \begin{align*}
        \bm x_k = \underset{\bm x \in \bm x_0 + \mathcal K_k(\mathbf A, \bm r_0)}{\arg \min } \| \mathbf A \bm x - \bm b \|_2.
    \end{align*}
\end{theorem}


\section{Examples of Krylov Subspace Methods}
We seek
$\bm x_k = \bm x_0 + \bm z_k$, where
\begin{align*}
    \bm{z}_{k} \in \mathcal{K}_{k}
    \quad\text{and}\quad
    \bm{r}_{0}-\mathbf{A} \bm{z}_{k} \perp \mathcal{L}_{k}.
\end{align*}
We know what the optimal criteris is, but how do we compute the optimal $\bm x_k$?

\newpage
 Computing the optimal $\bm x_k$ involves subproblems 
depending on the type of projection method being used.
Recall that $\mathbf A \mathbf Q_k = \mathbf Q_{k+1} \mathbf H_{k+1,k}$ as from 
Arnoldi process.
\begin{itemize}
    \item For orthogonal projection (CG), we have $\mathcal L_k = \mathcal K_k = \Range \mathbf Q_k$ and 
    $\bm x_k = \bm x_0 + \mathbf Q_k \bm y$. Then, 
    \begin{align*}
        \mathbf{Q}_{k}^{\top}\left(\bm{r}_{0}-\mathbf{A} \mathbf{Q}_{k} \bm{y}\right)=\mathbf{0} \Longleftrightarrow \mathbf{Q}_{k}^{\top} \mathbf{A} \mathbf{Q}_{k} \bm{y}=\mathbf{Q}_{k}^{\top} \bm{r}_{0} \Longleftrightarrow \mathbf{H}_{k} \bm{y}=\left\|\bm{r}_{0}\right\| \bm{e}_{1}.
    \end{align*}
    Note that the last equality holds because the first component
     of $\mathbf Q_k^\top \bm r_0$  is just $\langle \bm r_0 / \| \bm r_0 \|, \bm r_0\rangle$
    and its columns are orthogonal so the other elements evaluate to 0.
    \item For oblique projection (MINRES/GMRES), 
    $\mathcal L_k = \mathbf A\mathcal K_k = \mathbf A\Range \mathbf Q_k$ 
    and $\bm x_k = \bm x_0 + \mathbf Q_k \bm y$. Then, 
    \begin{align*}
        (\mathbf A \mathbf Q_k)^\top (\bm{r}_{0}-\mathbf{A} \mathbf{Q}_{k} \bm{y}) = \bm 0 \iff \mathbf Q_k^\top \mathbf A^\top \mathbf A \mathbf Q_k \bm y = \mathbf Q_k^\top \mathbf A^\top \bm r_0
    \end{align*}
    but note that this is exactly the normal equation for a least squares system. Thus, 
    \begin{align*}
        \bm y &= \underset{\bm y \in \mathbb R^k}{\arg \min} \frac{1}{2} \| \mathbf A \mathbf Q_k \bm y - \bm r_0 \|^2 \\ 
        &= \underset{\bm y \in \mathbb R^k}{\arg \min} \frac{1}{2} \| \mathbf H_{k+1,k} \bm y - \mathbf Q_{k+1}^\top\bm r_0 \|^2 \\ 
        &= \underset{\bm y \in \mathbb R^k}{\arg \min} \frac{1}{2} \Big\| \mathbf H_{k+1,k} \bm y - \|\bm r_0\| \bm e_1 \Big\|^2 
    \end{align*}
\end{itemize}
When $\mathbf A$ is symmetric, then so is $\mathbf Q_k^\top \mathbf A \mathbf Q_k$.
However, because this is upper Hessenberg and symmetric, it must be tridiagonal. 
It is denoted $\mathbf T_k$ instead of $\mathbf H_k$. In this case, the Arnoldi 
process is called the Lanczos process,
\begin{align*}
    \mathbf A \mathbf Q_{k} = \mathbf Q_{k+1} \mathbf T_{k+1,k}.
\end{align*}
The above subproblems can also  be rewritten with  $\mathbf T_k$ instead of $\mathbf H_k$.

In a previous section, we discussed the breakdown of the Arnoldi (or equivalently Lanczos)
method. Now, we show that this actually means we have found an exact solution.
\begin{proposition}
    If Arnoldi or Lanczos breaks down at step $t = t(\mathbf A, \bm r_0)$, 
    then the iterate $\bm x_t$ from any projection method onto 
    $\mathcal K_k(\mathbf A, \bm r_0)$ or $\mathbf A \mathcal K_k(\mathbf A, \bm r_0)$
    is the exact solution.
\end{proposition}

\section{GMRES/MINRES}
Suppose $\mathcal L_k = \mathbf A\mathcal K_k$ and $\mathbf A$ is not necessarily symmetric. 
Then, the subproblem of minimising
\begin{align*}
    \Big\|\mathbf{H}_{k+1,k} \bm{y}-\left\|\bm{r}_{0}\right\| \bm{e}_{1}\Big\|
\end{align*}
can be solved using the reduced QR factorisation $\mathbf{H}_{k+1,k} = \mathbf U_{k+1,k}\mathbf R_k$
by
\begin{align*}
    \bm y = \| \bm r_0 \| \mathbf R_k^{-1} \mathbf U_{k+1,k}^\top \bm e_1
\end{align*}
and the residual at the $k$-th iteration can be computed as 
\begin{align*}
    \| \bm b - \mathbf A \bm x_k \| = \| \bm r_0 \| \sqrt{1 - \|\mathbf U_{k+1,k}^\top \bm e_1 \|^2}.
\end{align*}
This expression for $\| \bm r_k\|$ is particularly useful because we do not need to compute  $\mathbf A \bm x_k$.
Careful inspection will show that each iteration only requires one matrix-vector product, 
$\mathbf A \bm q_k$.

In the case of $\mathbf A = \mathbf A^\top$ (symmetric), the method is 
called MINRES and 
\begin{align*}
    \Big\|\mathbf{T}_{k+1,k} \bm{y}-\left\|\bm{r}_{0}\right\| \bm{e}_{1}\Big\|
\end{align*}
can be solved similarly to above (but more efficiently, of course).

\end{document}